{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMING AND CLEANING THE DATASET\n",
    "This notebook shows the cleaning and transformation of the dataset. In order for our model to be able to give the highest accuracy at predicting the sentiment analysis, tokenization and removing stop words are crucial to the cleaning aspect of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "#Create a function to clean dataset, tokenize words and remove stopwords and punctuation\n",
    "def tokenize_words(text, stopwords, punctuation):\n",
    "    text = text.lower() \n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    text = re.sub(r\"[^a-z ]\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = []\n",
    "    for w in tokens:\n",
    "        if w not in stopwords and w not in punctuation:\n",
    "            filtered.append(w)\n",
    "    text = reduce((lambda x,y: x + \" \" + y), filtered)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Reviews (Cleaned)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>0</td>\n",
       "      <td>working one best shakespeare sources film mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>well tremors original started found movie quit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>0</td>\n",
       "      <td>ouch one bit painful sit cute amusing premise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>seen crappy movies life one must among worst d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Carriers follows the exploits of two guys and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>carriers follows exploits two guys two gals st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12495</td>\n",
       "      <td>My comments may be a bit of a spoiler, for wha...</td>\n",
       "      <td>1</td>\n",
       "      <td>comments may bit spoiler worth stop care enoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12496</td>\n",
       "      <td>The \"saucy\" misadventures of four au pairs who...</td>\n",
       "      <td>1</td>\n",
       "      <td>saucy misadventures four au pairs arrive londo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12497</td>\n",
       "      <td>Oh, those Italians! Assuming that movies about...</td>\n",
       "      <td>1</td>\n",
       "      <td>oh italians assuming movies aristocrats weird ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12498</td>\n",
       "      <td>Eight academy nominations? It's beyond belief....</td>\n",
       "      <td>1</td>\n",
       "      <td>eight academy nominations beyond belief think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12499</td>\n",
       "      <td>Not that I dislike childrens movies, but this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dislike childrens movies tearjerker redeeming ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Reviews  Encoding  \\\n",
       "0      Working with one of the best Shakespeare sourc...         0   \n",
       "1      Well...tremors I, the original started off in ...         0   \n",
       "2      Ouch! This one was a bit painful to sit throug...         0   \n",
       "3      I've seen some crappy movies in my life, but t...         0   \n",
       "4      Carriers follows the exploits of two guys and ...         0   \n",
       "...                                                  ...       ...   \n",
       "12495  My comments may be a bit of a spoiler, for wha...         1   \n",
       "12496  The \"saucy\" misadventures of four au pairs who...         1   \n",
       "12497  Oh, those Italians! Assuming that movies about...         1   \n",
       "12498  Eight academy nominations? It's beyond belief....         1   \n",
       "12499  Not that I dislike childrens movies, but this ...         1   \n",
       "\n",
       "                                       Reviews (Cleaned)  \n",
       "0      working one best shakespeare sources film mana...  \n",
       "1      well tremors original started found movie quit...  \n",
       "2      ouch one bit painful sit cute amusing premise ...  \n",
       "3      seen crappy movies life one must among worst d...  \n",
       "4      carriers follows exploits two guys two gals st...  \n",
       "...                                                  ...  \n",
       "12495  comments may bit spoiler worth stop care enoug...  \n",
       "12496  saucy misadventures four au pairs arrive londo...  \n",
       "12497  oh italians assuming movies aristocrats weird ...  \n",
       "12498  eight academy nominations beyond belief think ...  \n",
       "12499  dislike childrens movies tearjerker redeeming ...  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_tokenize accepts a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words = [re.sub(r\"[^a-z ]\", \"\", w) for w in stop_words]\n",
    "\n",
    "#Read in .txt file\n",
    "test_neg_path = \"../train_neg.txt\" \n",
    "test_pos_path = \"../train_pos.txt\" #need to change this\n",
    "\n",
    "test_neg_df = pd.read_table(test_neg_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "test_pos_df = pd.read_table(test_pos_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "\n",
    "#Encoding each review with 0 for negative and 1 for positive \n",
    "test_neg_df['Encoding'] = 0\n",
    "test_pos_df['Encoding'] = 1\n",
    "\n",
    "#Concatenating both negative and positive reviews to insert into a dataframe\n",
    "test_df = pd.concat([test_neg_df, test_pos_df])\n",
    "\n",
    "#Tokenize words, removing stop words, removing punctuation and creating the dataframe\n",
    "test_df['Reviews (Cleaned)'] = test_df['Reviews'].apply(tokenize_words, args=(stop_words, punctuation))\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a for loop to find the word frequency for tokenized words for visualization purposes\n",
    "wordfreq = {}\n",
    "for sentence in test_df['Reviews (Cleaned)']:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 200 most frequent words \n",
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "#Create dataframe and save into csv \n",
    "df_new = pd.DataFrame.from_dict(wordfreq, orient=\"index\")\n",
    "df_new.to_csv('word_frequency.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING OUR MODEL \n",
    "The code below shows the steps of how we tested our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_np = test_df['Reviews (Cleaned)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizing our words\n",
    "CV = CountVectorizer(input=\"content\", lowercase=False)\n",
    "CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardize the data\n",
    "cv_matrix = CV.fit_transform(reviews_np)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaahhhhhhh</th>\n",
       "      <th>aaaarrgh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aaaugh</th>\n",
       "      <th>aaawwwwnnn</th>\n",
       "      <th>...</th>\n",
       "      <th>zyuranger</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzzz</th>\n",
       "      <th>zzzzz</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 53052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aa  aaa  aaaaaaah  aaaaah  aaaahhhhhhh  aaaarrgh  aaah  aaargh  aaaugh  \\\n",
       "0     0    0         0       0            0         0     0       0       0   \n",
       "1     0    0         0       0            0         0     0       0       0   \n",
       "2     0    0         0       0            0         0     0       0       0   \n",
       "3     0    0         0       0            0         0     0       0       0   \n",
       "4     0    0         0       0            0         0     0       0       0   \n",
       "..   ..  ...       ...     ...          ...       ...   ...     ...     ...   \n",
       "495   0    0         0       0            0         0     0       0       0   \n",
       "496   0    0         0       0            0         0     0       0       0   \n",
       "497   0    0         0       0            0         0     0       0       0   \n",
       "498   0    0         0       0            0         0     0       0       0   \n",
       "499   0    0         0       0            0         0     0       0       0   \n",
       "\n",
       "     aaawwwwnnn  ...  zyuranger  zz  zzzz  zzzzz  zzzzzzzz  zzzzzzzzzzzz  \\\n",
       "0             0  ...          0   0     0      0         0             0   \n",
       "1             0  ...          0   0     0      0         0             0   \n",
       "2             0  ...          0   0     0      0         0             0   \n",
       "3             0  ...          0   0     0      0         0             0   \n",
       "4             0  ...          0   0     0      0         0             0   \n",
       "..          ...  ...        ...  ..   ...    ...       ...           ...   \n",
       "495           0  ...          0   0     0      0         0             0   \n",
       "496           0  ...          0   0     0      0         0             0   \n",
       "497           0  ...          0   0     0      0         0             0   \n",
       "498           0  ...          0   0     0      0         0             0   \n",
       "499           0  ...          0   0     0      0         0             0   \n",
       "\n",
       "     zzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                0                                0   \n",
       "1                0                                0   \n",
       "2                0                                0   \n",
       "3                0                                0   \n",
       "4                0                                0   \n",
       "..             ...                              ...   \n",
       "495              0                                0   \n",
       "496              0                                0   \n",
       "497              0                                0   \n",
       "498              0                                0   \n",
       "499              0                                0   \n",
       "\n",
       "     zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "..                                         ...   \n",
       "495                                          0   \n",
       "496                                          0   \n",
       "497                                          0   \n",
       "498                                          0   \n",
       "499                                          0   \n",
       "\n",
       "     zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
       "0                                               0  \n",
       "1                                               0  \n",
       "2                                               0  \n",
       "3                                               0  \n",
       "4                                               0  \n",
       "..                                            ...  \n",
       "495                                             0  \n",
       "496                                             0  \n",
       "497                                             0  \n",
       "498                                             0  \n",
       "499                                             0  \n",
       "\n",
       "[500 rows x 53052 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Depicts term frequency vector for each review (bag of words)\n",
    "vocab = CV.get_feature_names()\n",
    "df_reviews = pd.DataFrame(cv_matrix, columns=vocab)\n",
    "df_reviews.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new column in dataframe to match each matrix to its corresponding review\n",
    "test_df['matrix'] = list(cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.linear_model.logistic.LogisticRegression"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables to train dataset\n",
    "X_train = cv_matrix\n",
    "y_train = test_df['Encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model variable\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angelicabalma/anaconda3/envs/PythonData1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit linear model \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check model accuracy \n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
