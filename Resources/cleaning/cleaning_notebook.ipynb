{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMING AND CLEANING THE DATASET\n",
    "This notebook shows the cleaning and transformation of the dataset and testing our Machine Learning model. In order for our model to be able to give the highest accuracy at predicting the sentiment analysis, tokenization and removing stop words are crucial to the cleaning aspect of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "#Create a function to clean dataset, tokenize words and remove stopwords and punctuation\n",
    "def tokenize_words(text, stopwords, punctuation):\n",
    "    text = text.lower() \n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    text = re.sub(r\"[^a-z ]\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = []\n",
    "    for w in tokens:\n",
    "        if w not in stopwords and w not in punctuation:\n",
    "            filtered.append(w)\n",
    "    text = reduce((lambda x,y: x + \" \" + y), filtered)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Reviews (Cleaned)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>0</td>\n",
       "      <td>working one best shakespeare sources film mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>well tremors original started found movie quit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>0</td>\n",
       "      <td>ouch one bit painful sit cute amusing premise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>seen crappy movies life one must among worst d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Carriers follows the exploits of two guys and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>carriers follows exploits two guys two gals st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12495</td>\n",
       "      <td>About a year ago I finally gave up on American...</td>\n",
       "      <td>1</td>\n",
       "      <td>year ago finally gave american television thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12496</td>\n",
       "      <td>When I saw the elaborate DVD box for this and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>saw elaborate dvd box dreadful red queen figur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12497</td>\n",
       "      <td>Last November, I had a chance to see this film...</td>\n",
       "      <td>1</td>\n",
       "      <td>last november chance see film reno film festiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12498</td>\n",
       "      <td>Great movie -I loved it. Great editing and use...</td>\n",
       "      <td>1</td>\n",
       "      <td>great movie loved great editing use soundtrack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12499</td>\n",
       "      <td>Enchanted April is a tone poem, an impressioni...</td>\n",
       "      <td>1</td>\n",
       "      <td>enchanted april tone poem impressionist painti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Reviews  Encoding  \\\n",
       "0      Working with one of the best Shakespeare sourc...         0   \n",
       "1      Well...tremors I, the original started off in ...         0   \n",
       "2      Ouch! This one was a bit painful to sit throug...         0   \n",
       "3      I've seen some crappy movies in my life, but t...         0   \n",
       "4      Carriers follows the exploits of two guys and ...         0   \n",
       "...                                                  ...       ...   \n",
       "12495  About a year ago I finally gave up on American...         1   \n",
       "12496  When I saw the elaborate DVD box for this and ...         1   \n",
       "12497  Last November, I had a chance to see this film...         1   \n",
       "12498  Great movie -I loved it. Great editing and use...         1   \n",
       "12499  Enchanted April is a tone poem, an impressioni...         1   \n",
       "\n",
       "                                       Reviews (Cleaned)  \n",
       "0      working one best shakespeare sources film mana...  \n",
       "1      well tremors original started found movie quit...  \n",
       "2      ouch one bit painful sit cute amusing premise ...  \n",
       "3      seen crappy movies life one must among worst d...  \n",
       "4      carriers follows exploits two guys two gals st...  \n",
       "...                                                  ...  \n",
       "12495  year ago finally gave american television thou...  \n",
       "12496  saw elaborate dvd box dreadful red queen figur...  \n",
       "12497  last november chance see film reno film festiv...  \n",
       "12498  great movie loved great editing use soundtrack...  \n",
       "12499  enchanted april tone poem impressionist painti...  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_tokenize accepts a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words = [re.sub(r\"[^a-z ]\", \"\", w) for w in stop_words]\n",
    "\n",
    "#Read in .txt file\n",
    "test_neg_path = \"../train_neg.txt\" \n",
    "test_pos_path = \"../train_pos.txt\" #need to change this\n",
    "\n",
    "test_neg_df = pd.read_table(test_neg_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "test_pos_df = pd.read_table(test_pos_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "\n",
    "#Encoding each review with 0 for negative and 1 for positive \n",
    "test_neg_df['Encoding'] = 0\n",
    "test_pos_df['Encoding'] = 1\n",
    "\n",
    "#Concatenating both negative and positive reviews to insert into a dataframe\n",
    "test_df = pd.concat([test_neg_df, test_pos_df])\n",
    "\n",
    "#Tokenize words, removing stop words, removing punctuation and creating the dataframe\n",
    "test_df['Reviews (Cleaned)'] = test_df['Reviews'].apply(tokenize_words, args=(stop_words, punctuation))\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a for loop to find the word frequency for tokenized words for visualization purposes\n",
    "wordfreq = {}\n",
    "for sentence in test_df['Reviews (Cleaned)']:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 200 most frequent words \n",
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "#Create dataframe and save into csv \n",
    "df_new = pd.DataFrame.from_dict(wordfreq, orient=\"index\")\n",
    "df_new.to_csv('word_frequency.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING OUR MODEL \n",
    "The code below shows the steps of how we tested our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_np = test_df['Reviews (Cleaned)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizing our words\n",
    "CV = CountVectorizer(input=\"content\", lowercase=False)\n",
    "CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardize the data\n",
    "cv_matrix = CV.fit_transform(reviews_np)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaatch</th>\n",
       "      <th>aaaahhhhhhh</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaaarrgh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aaargh</th>\n",
       "      <th>...</th>\n",
       "      <th>zyuranger</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzzz</th>\n",
       "      <th>zzzzz</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 73081 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aa  aaa  aaaaaaah  aaaaah  aaaaatch  aaaahhhhhhh  aaaand  aaaarrgh  aaah  \\\n",
       "0     0    0         0       0         0            0       0         0     0   \n",
       "1     0    0         0       0         0            0       0         0     0   \n",
       "2     0    0         0       0         0            0       0         0     0   \n",
       "3     0    0         0       0         0            0       0         0     0   \n",
       "4     0    0         0       0         0            0       0         0     0   \n",
       "..   ..  ...       ...     ...       ...          ...     ...       ...   ...   \n",
       "495   0    0         0       0         0            0       0         0     0   \n",
       "496   0    0         0       0         0            0       0         0     0   \n",
       "497   0    0         0       0         0            0       0         0     0   \n",
       "498   0    0         0       0         0            0       0         0     0   \n",
       "499   0    0         0       0         0            0       0         0     0   \n",
       "\n",
       "     aaargh  ...  zyuranger  zz  zzzz  zzzzz  zzzzzzzz  zzzzzzzzzzzz  \\\n",
       "0         0  ...          0   0     0      0         0             0   \n",
       "1         0  ...          0   0     0      0         0             0   \n",
       "2         0  ...          0   0     0      0         0             0   \n",
       "3         0  ...          0   0     0      0         0             0   \n",
       "4         0  ...          0   0     0      0         0             0   \n",
       "..      ...  ...        ...  ..   ...    ...       ...           ...   \n",
       "495       0  ...          0   0     0      0         0             0   \n",
       "496       0  ...          0   0     0      0         0             0   \n",
       "497       0  ...          0   0     0      0         0             0   \n",
       "498       0  ...          0   0     0      0         0             0   \n",
       "499       0  ...          0   0     0      0         0             0   \n",
       "\n",
       "     zzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                0                                0   \n",
       "1                0                                0   \n",
       "2                0                                0   \n",
       "3                0                                0   \n",
       "4                0                                0   \n",
       "..             ...                              ...   \n",
       "495              0                                0   \n",
       "496              0                                0   \n",
       "497              0                                0   \n",
       "498              0                                0   \n",
       "499              0                                0   \n",
       "\n",
       "     zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "..                                         ...   \n",
       "495                                          0   \n",
       "496                                          0   \n",
       "497                                          0   \n",
       "498                                          0   \n",
       "499                                          0   \n",
       "\n",
       "     zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
       "0                                               0  \n",
       "1                                               0  \n",
       "2                                               0  \n",
       "3                                               0  \n",
       "4                                               0  \n",
       "..                                            ...  \n",
       "495                                             0  \n",
       "496                                             0  \n",
       "497                                             0  \n",
       "498                                             0  \n",
       "499                                             0  \n",
       "\n",
       "[500 rows x 73081 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Depicts term frequency vector for each review (bag of words)\n",
    "vocab = CV.get_feature_names()\n",
    "df_reviews = pd.DataFrame(cv_matrix, columns=vocab)\n",
    "df_reviews.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new column in dataframe to match each matrix to its corresponding review\n",
    "test_df['matrix'] = list(cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.linear_model.logistic.LogisticRegression"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables to train dataset\n",
    "X_train = cv_matrix\n",
    "y_train = test_df['Encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model variable\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angelicabalma/anaconda3/envs/PythonData1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit linear model \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99824"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check model accuracy \n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight of a word\n",
    "In this section we tried to see the weight of each word in each review in accordance with all other reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.95057656e-01, -1.21714029e-01, -2.21655083e-07, ...,\n",
       "         3.20608939e-06,  3.20608939e-06,  3.20608939e-06]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cheacking coefficent of words\n",
    "model_example = model.coef_\n",
    "#model_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One example of a review\n",
    "review_ex = test_df.iloc[3620]\n",
    "review_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.537607604777733"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(review_ex['matrix'], model_example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = [ model_example[0][i] * review_ex['matrix'][i] for i in range(73081)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights</th>\n",
       "      <th>Vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>aaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>aaaaaaah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aaaaah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aaaaatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73076</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>zzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73077</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>zzzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73081 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Weights                                         Vocab\n",
       "0         -0.0                                            aa\n",
       "1         -0.0                                           aaa\n",
       "2         -0.0                                      aaaaaaah\n",
       "3          0.0                                        aaaaah\n",
       "4          0.0                                      aaaaatch\n",
       "...        ...                                           ...\n",
       "73076     -0.0                                  zzzzzzzzzzzz\n",
       "73077     -0.0                                 zzzzzzzzzzzzz\n",
       "73078      0.0               zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
       "73079      0.0     zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
       "73080      0.0  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
       "\n",
       "[73081 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look = pd.DataFrame({\"Weights\": strength, \"Vocab\": vocab})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_weights = look.loc[look['Weights'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Weights    1.61446\n",
       "Vocab         work\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max weight \n",
    "review_weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Weights   -1.78048\n",
       "Vocab       across\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Min Weight\n",
    "review_weights.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.537607604777733"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Weight SUM\n",
    "review_weights[\"Weights\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file as CSV\n",
    "#review_weights.to_csv(\"example_weights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9262405, 0.0737595]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FINAL: Model correctly predicts negative review and the probability of a positive review is 0.73\n",
    "model.predict_proba(review_ex['matrix'].reshape(1,-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
