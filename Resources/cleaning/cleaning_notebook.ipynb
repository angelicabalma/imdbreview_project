{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMING AND CLEANING THE DATASET\n",
    "This notebook shows the cleaning and transformation of the dataset. In order for our model to be able to give the highest accuracy at predicting the sentiment analysis, tokenization and removing stop words are crucial to the cleaning aspect of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "#Create a function to clean dataset, tokenize words and remove stopwords and punctuation\n",
    "def tokenize_words(text, stopwords, punctuation):\n",
    "    text = text.lower() \n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    text = re.sub(r\"[^a-z ]\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = []\n",
    "    for w in tokens:\n",
    "        if w not in stopwords and w not in punctuation:\n",
    "            filtered.append(w)\n",
    "    text = reduce((lambda x,y: x + \" \" + y), filtered)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Reviews (Cleaned)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>0</td>\n",
       "      <td>working one best shakespeare sources film mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>well tremors original started found movie quit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>0</td>\n",
       "      <td>ouch one bit painful sit cute amusing premise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>seen crappy movies life one must among worst d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Carriers follows the exploits of two guys and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>carriers follows exploits two guys two gals st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12495</td>\n",
       "      <td>About a year ago I finally gave up on American...</td>\n",
       "      <td>1</td>\n",
       "      <td>year ago finally gave american television thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12496</td>\n",
       "      <td>When I saw the elaborate DVD box for this and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>saw elaborate dvd box dreadful red queen figur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12497</td>\n",
       "      <td>Last November, I had a chance to see this film...</td>\n",
       "      <td>1</td>\n",
       "      <td>last november chance see film reno film festiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12498</td>\n",
       "      <td>Great movie -I loved it. Great editing and use...</td>\n",
       "      <td>1</td>\n",
       "      <td>great movie loved great editing use soundtrack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12499</td>\n",
       "      <td>Enchanted April is a tone poem, an impressioni...</td>\n",
       "      <td>1</td>\n",
       "      <td>enchanted april tone poem impressionist painti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Reviews  Encoding  \\\n",
       "0      Working with one of the best Shakespeare sourc...         0   \n",
       "1      Well...tremors I, the original started off in ...         0   \n",
       "2      Ouch! This one was a bit painful to sit throug...         0   \n",
       "3      I've seen some crappy movies in my life, but t...         0   \n",
       "4      Carriers follows the exploits of two guys and ...         0   \n",
       "...                                                  ...       ...   \n",
       "12495  About a year ago I finally gave up on American...         1   \n",
       "12496  When I saw the elaborate DVD box for this and ...         1   \n",
       "12497  Last November, I had a chance to see this film...         1   \n",
       "12498  Great movie -I loved it. Great editing and use...         1   \n",
       "12499  Enchanted April is a tone poem, an impressioni...         1   \n",
       "\n",
       "                                       Reviews (Cleaned)  \n",
       "0      working one best shakespeare sources film mana...  \n",
       "1      well tremors original started found movie quit...  \n",
       "2      ouch one bit painful sit cute amusing premise ...  \n",
       "3      seen crappy movies life one must among worst d...  \n",
       "4      carriers follows exploits two guys two gals st...  \n",
       "...                                                  ...  \n",
       "12495  year ago finally gave american television thou...  \n",
       "12496  saw elaborate dvd box dreadful red queen figur...  \n",
       "12497  last november chance see film reno film festiv...  \n",
       "12498  great movie loved great editing use soundtrack...  \n",
       "12499  enchanted april tone poem impressionist painti...  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_tokenize accepts a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words = [re.sub(r\"[^a-z ]\", \"\", w) for w in stop_words]\n",
    "\n",
    "#Read in .txt file\n",
    "test_neg_path = \"../train_neg.txt\" \n",
    "test_pos_path = \"../train_pos.txt\" #need to change this\n",
    "\n",
    "test_neg_df = pd.read_table(test_neg_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "test_pos_df = pd.read_table(test_pos_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "\n",
    "#Encoding each review with 0 for negative and 1 for positive \n",
    "test_neg_df['Encoding'] = 0\n",
    "test_pos_df['Encoding'] = 1\n",
    "\n",
    "#Concatenating both negative and positive reviews to insert into a dataframe\n",
    "test_df = pd.concat([test_neg_df, test_pos_df])\n",
    "\n",
    "#Tokenize words, removing stop words, removing punctuation and creating the dataframe\n",
    "test_df['Reviews (Cleaned)'] = test_df['Reviews'].apply(tokenize_words, args=(stop_words, punctuation))\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7a9a328de1d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwordfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviews (Cleaned)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PythonData1/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/envs/PythonData1/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/envs/PythonData1/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PythonData1/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Creating a for loop to find the word frequency for tokenized words for visualization purposes\n",
    "wordfreq = {}\n",
    "for sentence in test_df['Reviews (Cleaned)']:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 200 most frequent words \n",
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "#Create dataframe and save into csv \n",
    "df_new = pd.DataFrame.from_dict(wordfreq, orient=\"index\")\n",
    "df_new.to_csv('word_frequency.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING OUR MODEL \n",
    "The code below shows the steps of how we tested our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_np = test_df['Reviews (Cleaned)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizing our words\n",
    "CV = CountVectorizer(input=\"content\", lowercase=False)\n",
    "CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardize the data\n",
    "cv_matrix = CV.fit_transform(reviews_np)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaatch</th>\n",
       "      <th>aaaahhhhhhh</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaaarrgh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aaargh</th>\n",
       "      <th>...</th>\n",
       "      <th>zyuranger</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzzz</th>\n",
       "      <th>zzzzz</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 73081 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aa  aaa  aaaaaaah  aaaaah  aaaaatch  aaaahhhhhhh  aaaand  aaaarrgh  aaah  \\\n",
       "0     0    0         0       0         0            0       0         0     0   \n",
       "1     0    0         0       0         0            0       0         0     0   \n",
       "2     0    0         0       0         0            0       0         0     0   \n",
       "3     0    0         0       0         0            0       0         0     0   \n",
       "4     0    0         0       0         0            0       0         0     0   \n",
       "..   ..  ...       ...     ...       ...          ...     ...       ...   ...   \n",
       "495   0    0         0       0         0            0       0         0     0   \n",
       "496   0    0         0       0         0            0       0         0     0   \n",
       "497   0    0         0       0         0            0       0         0     0   \n",
       "498   0    0         0       0         0            0       0         0     0   \n",
       "499   0    0         0       0         0            0       0         0     0   \n",
       "\n",
       "     aaargh  ...  zyuranger  zz  zzzz  zzzzz  zzzzzzzz  zzzzzzzzzzzz  \\\n",
       "0         0  ...          0   0     0      0         0             0   \n",
       "1         0  ...          0   0     0      0         0             0   \n",
       "2         0  ...          0   0     0      0         0             0   \n",
       "3         0  ...          0   0     0      0         0             0   \n",
       "4         0  ...          0   0     0      0         0             0   \n",
       "..      ...  ...        ...  ..   ...    ...       ...           ...   \n",
       "495       0  ...          0   0     0      0         0             0   \n",
       "496       0  ...          0   0     0      0         0             0   \n",
       "497       0  ...          0   0     0      0         0             0   \n",
       "498       0  ...          0   0     0      0         0             0   \n",
       "499       0  ...          0   0     0      0         0             0   \n",
       "\n",
       "     zzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                0                                0   \n",
       "1                0                                0   \n",
       "2                0                                0   \n",
       "3                0                                0   \n",
       "4                0                                0   \n",
       "..             ...                              ...   \n",
       "495              0                                0   \n",
       "496              0                                0   \n",
       "497              0                                0   \n",
       "498              0                                0   \n",
       "499              0                                0   \n",
       "\n",
       "     zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "..                                         ...   \n",
       "495                                          0   \n",
       "496                                          0   \n",
       "497                                          0   \n",
       "498                                          0   \n",
       "499                                          0   \n",
       "\n",
       "     zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
       "0                                               0  \n",
       "1                                               0  \n",
       "2                                               0  \n",
       "3                                               0  \n",
       "4                                               0  \n",
       "..                                            ...  \n",
       "495                                             0  \n",
       "496                                             0  \n",
       "497                                             0  \n",
       "498                                             0  \n",
       "499                                             0  \n",
       "\n",
       "[500 rows x 73081 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Depicts term frequency vector for each review (bag of words)\n",
    "vocab = CV.get_feature_names()\n",
    "df_reviews = pd.DataFrame(cv_matrix, columns=vocab)\n",
    "df_reviews.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new column in dataframe to match each matrix to its corresponding review\n",
    "test_df['matrix'] = list(cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.linear_model.logistic.LogisticRegression"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables to train dataset\n",
    "X_train = cv_matrix\n",
    "y_train = test_df['Encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model variable\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angelicabalma/anaconda3/envs/PythonData1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit linear model \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99824"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check model accuracy \n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reviews              When childhood memory tells you this was a sca...\n",
       "Encoding                                                             0\n",
       "Reviews (Cleaned)    childhood memory tells scary movie touch go wh...\n",
       "matrix               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: 3620, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_ex = test_df.iloc[3620]\n",
    "# review_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_example = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.95057656e-01, -1.21714029e-01, -2.21655083e-07, ...,\n",
       "         3.20608939e-06,  3.20608939e-06,  3.20608939e-06]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73081"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(review_ex['matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.537607604777733"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.dot(review_ex['matrix'], model_example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strength = [ model_example[0][i] * review_ex['matrix'][i] for i in range(73081)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights</th>\n",
       "      <th>Vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>aaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>aaaaaaah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aaaaah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aaaaatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73076</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>zzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73077</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>zzzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73081 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Weights                                         Vocab\n",
       "0         -0.0                                            aa\n",
       "1         -0.0                                           aaa\n",
       "2         -0.0                                      aaaaaaah\n",
       "3          0.0                                        aaaaah\n",
       "4          0.0                                      aaaaatch\n",
       "...        ...                                           ...\n",
       "73076     -0.0                                  zzzzzzzzzzzz\n",
       "73077     -0.0                                 zzzzzzzzzzzzz\n",
       "73078      0.0               zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
       "73079      0.0     zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
       "73080      0.0  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
       "\n",
       "[73081 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look = pd.DataFrame({\"Weights\": strength, \"Vocab\": vocab})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_weights = look.loc[look['Weights'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Weights    1.61446\n",
       "Vocab         work\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Weights   -1.78048\n",
       "Vocab       across\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_weights.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.537607604777733"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_weights[\"Weights\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_weights.to_csv(\"example_weights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When childhood memory tells you this was a scary movie; it\\'s touch and go whether you should revisit it. Anyway, I remembered a scary scene involving a homeless person and a cool villain played by Jeff Kober.<br /><br />\"The First Power\" is not a very good movie, sad to say. It\\'s chock full of those cop clichÃ©s and a very poor script with holes a truck could drive through (along with countless convenient \"twists\" that help the story run along). Lou Diamond Phillips is the over-confident bad ass cop who sends baddie serial killer Kober to the gas chamber only to find out he was a minion of Satan himself and now has the power of resurrection along with the power of possessing every weak minded person who he comes across. Through in the mix a very poorly realized psychic who helps with the case.<br /><br />Ahhh, this is trash. But enjoyable as such, especially if you have fond memories of it. It scared me as a kid and that scene with the homeless person is still pretty good. As for any kind of logic here; forget it, just about every scenario is thrown in for good measure and you end up with a cross between a Steven Segal action flick and a 70\\'s demonic flick. And who on earth thought it was a good idea to cast Lou Diamond Phillips in the lead here? Needless to say he\\'s not convincing at all but he tries his best and I\\'ve never had the problem with the guy so many reviewers here seem to have. As for Tracy Griffith as the psychic, the less said the better. But Kober is pretty good as the killer; always liked that actor.<br /><br />\"The First Power\" may be just what the doctor ordered after a hard day\\'s work and a \"brain switch-off\" is needed. Beer will most likely enhance the viewing experience and I\\'ll definitely have loads of it the next time I give this movie a spin. All in all; not a good flick but a somewhat guilty pleasure for nostalgic fans who were easily scared as kids. \"See you around, buddy boy\"!'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_ex['Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reviews              When childhood memory tells you this was a sca...\n",
       "Encoding                                                             0\n",
       "Reviews (Cleaned)    childhood memory tells scary movie touch go wh...\n",
       "matrix               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: 3620, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9262405, 0.0737595]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(review_ex['matrix'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
