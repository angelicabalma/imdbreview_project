{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request as req\n",
    "# import tarfile\n",
    "# import os\n",
    "\n",
    "# imdb_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# save_filename = \"aclImdb_v1.tar.gz\"\n",
    "# if not os.path.exists(save_filename):\n",
    "#     req.urlretrieve(imdb_url, save_filename)\n",
    "    \n",
    "# imdb_folder = \"aclImdb\"\n",
    "# if not os.path.exists(imdb_folder):\n",
    "#     with tarfile.open(save_filename) as tar:\n",
    "#         tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# def get_reviews(data_folder=\"/train\"):\n",
    "#     reviews = []\n",
    "#     labels = []\n",
    "#     for index,sentiment in enumerate([\"/neg/\", \"/pos/\"]):\n",
    "#         path = imdb_folder + data_folder + sentiment\n",
    "#         for filename in sorted(os.listdir(path)):\n",
    "#             with open(path + filename, 'r') as f:\n",
    "#                 review = f.read()\n",
    "#                 review = review.lower()\n",
    "#                 review = review.replace(\"<br />\", \" \")\n",
    "#                 review = re.sub(r\"[^a-z ]\", \" \", review)\n",
    "#                 review = re.sub(r\" +\", \" \", review)\n",
    "#                 review = review.split(\" \")\n",
    "#                 reviews.append(review)\n",
    "                \n",
    "#                 label = [0, 0]\n",
    "#                 label[index] = 1\n",
    "#                 labels.append(label)\n",
    "                \n",
    "#     return reviews, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_reviews, train_labels = get_reviews()\n",
    "# print(len(train_reviews))\n",
    "# print(train_reviews[0])\n",
    "# print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"I love the rain and the sun. It makes me feel whole and grown. Who knew that life was mysterious like your mom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words():\n",
    "    review_tokenized = []\n",
    "    review = word_tokenize()\n",
    "    reviews.append(review_tokenized)\n",
    "    return review_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_words(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_words(str):\n",
    "    f = StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        for i in str:\n",
    "            regex_of_word = re.findall('([\\w]{0,})', i)\n",
    "            regex_of_word = [x for x in regex_of_word if x is not '']\n",
    "            for word in regex_of_word:\n",
    "                print(regex_of_word)\n",
    "        words = (f.getvalue()).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(['Mary', 'Jack', 'Jill', 'i', 'it'])\n",
    "\n",
    "mod_example = []\n",
    "for sentence in example:\n",
    "    words = sentence.split()\n",
    "    # Optionally sort out some words\n",
    "    for word in words:\n",
    "        if word in exclude:\n",
    "            words.remove(word)\n",
    "    mod_example.append('\\'' + '\\' \\''.join(words) + '\\'')\n",
    "\n",
    "print mod_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords \n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# text = \"I love the rain and the sun. It makes me feel whole and grown. Who knew that life was mysterious like your mom.\"\n",
    "\n",
    "# stop_words = set(stopwords.words('english')) \n",
    "# word_tokens = word_tokenize(text) \n",
    "# filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "# filtered_sentence = [] \n",
    "  \n",
    "\n",
    "# # def tokenize_words():\n",
    "# #     for wordings in Text:\n",
    "# #         words = word_tokenize(wordings)\n",
    "# #         for word in words:\n",
    "# #             stop = stopwords.words('english')\n",
    "# #             if word in stop:\n",
    "# #                 words.remove(word)\n",
    "# #         mod_example.append(\"\\\" + '\\' \\''.join(words) + '\\'') \n",
    "        \n",
    "\n",
    "# def tokenize_words():\n",
    "#     for w in word_tokens: \n",
    "#         if w not in stop_words: \n",
    "#             filtered_sentence.append(w)\n",
    "#     print(word_tokens)\n",
    "#     print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "\n",
    "text = \"I love the rain and the sun. It makes me feel whole and grown. Who knew that life was mysterious like your mom.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(text) \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRECT CODE \n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def tokenize_words(text, stopwords, punctuation):\n",
    "    text = text.lower() \n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    text = re.sub(r\"[^a-z ]\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "#     tokens = word_tokenize(text)\n",
    "#     filtered = []\n",
    "#     for w in tokens:\n",
    "#         if w not in stopwords and w not in punctuation:\n",
    "#             filtered.append(w)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#word_tokenize accepts a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english')) \n",
    "# file1 = open(\"../test_neg.txt\") \n",
    "# line = file1.read()# Use this to read file content as a stream: \n",
    "# words = line.split() \n",
    "# for r in words: \n",
    "#     if not r in stop_words: \n",
    "#         appendFile = open('filteredtext.txt','a') \n",
    "#         appendFile.write(\" \"+r) \n",
    "#         appendFile.close()\n",
    "        \n",
    "with open(\"../test_neg.txt\") as readfile:\n",
    "    for i in range(10):\n",
    "        line = readfile.readline()\n",
    "        filtered = tokenize_words(line, stop_words, string.punctuation)\n",
    "        print(filtered)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_path = \"../test_neg.txt\"\n",
    "reviews = pd.read_table(test_neg_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Reviews (filtered)'] = reviews['Reviews'].apply(tokenize_words, args=(stop_words, punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for token in reviews['Reviews']:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_np = reviews['Reviews (filtered)'].to_numpy()\n",
    "reviews_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in reviews_np:\n",
    "    for token in sentence:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer(input=\"content\", lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CV.fit_transform(reviews_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_path = \"../train_neg.txt\"\n",
    "test_pos_path = \"../train_pos.txt\"\n",
    "reviews1 = pd.read_table(test_neg_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "reviews1['Encoding'] = 0\n",
    "reviews1.head()\n",
    "\n",
    "\n",
    "# df = pd.concat('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews2 = pd.read_table(test_pos_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "reviews2['Encoding'] = 1\n",
    "reviews2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([reviews1, reviews2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews_cleaned'] = \\\n",
    "    df['Reviews'].apply(tokenize_words, args=(stop_words, punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer(input=\"content\", lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CV.fit_transform(df['reviews_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['matrixdf'] = list(vocab.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vocab.toarray()\n",
    "y_train = df['Encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_path = \"../test_neg.txt\"\n",
    "test_pos_path = \"../test_pos.txt\"\n",
    "\n",
    "test_neg_df = pd.read_table(test_neg_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "test_pos_df = pd.read_table(test_pos_path, sep=\"\\n\", header=None, names=['Reviews'])\n",
    "\n",
    "test_neg_df['Encoding'] = 0\n",
    "test_pos_df['Encoding'] = 1\n",
    "\n",
    "test_df = pd.concat([test_neg_df, test_pos_df])\n",
    "\n",
    "test_df['Reviews (Cleaned)'] = test_df['Reviews'].apply(tokenize_words, args=(stop_words, punctuation))\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = CV.transform(test_df['Reviews (Cleaned)']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['Encoding']\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test[12444].reshape(1,-1))\n",
    "print(f\"Review: {test_df.iloc[12444,0]}\")\n",
    "print(f\"Actual Classifier: {test_df.iloc[12444, 1]}\")\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
